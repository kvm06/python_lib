1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?

У метрик качества классификации (recall, f1, precision) имеется атрибут average, который определяет способ усреднения результатов метрик. 
micro - рассчитывает метрики глобально, подсчитав общее количество истинных положительных, ложных отрицательных и ложных положительных результатов. 
macro - вычисляет метрики для каждой метки и находит их невзвешенное среднее. Различие в количестве меток не берется во внимание. 
weighted - вычисляет метрики для каждой метки и находит их взвешенное среднее значение, т.е. с учетом количества истинных значений для каждой метки. 

2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
XGBoost - обучается на ошибках предыдущих бустингов. Для понимания какой вопрос является хорошим, в XGBoost используется похожесть, которая рассчитывается по формуле. 
LightGBM - эта модель обучается быстрее всех бустингов. Для снижения времени обучения, обучение моделей происходит не на всей выборке, а на подвыборке и сначала обучаются на объектах с большими ошибками. 
Также для ускорения происходит объединение признаков (feature bundling), Таким образом будет меньше признаков. В отличие от других бустингов LightGBM строится не по уровням, а по листьям. 
CatBoost - использует для обучения симметричные деревья. Т.е. на каждом уровне задается один и тот же вопрос для двух ветвей дерева. Хорошо подходит для поиска за счет высокой скорости. 
Другой особенностью является, снижает переобучение за счет того, что на каждой итерации обучается несколько моделей за раз, а также высокая случайность выборки. CatBoost хорошо работает с категориальными признаками. 
